import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import numpy as np
import pickle
from PIL import Image
import os

# --- Thiáº¿t láº­p trang ---
st.set_page_config(page_title="TrÃ¬nh táº¡o ChÃº thÃ­ch áº¢nh", layout="centered")

# --- Táº£i Model vÃ  Tokenizer (Sá»­ dá»¥ng cache Ä‘á»ƒ khÃ´ng táº£i láº¡i má»—i láº§n) ---
@st.cache_resource
def load_all_models():
    """
    Táº£i táº¥t cáº£ cÃ¡c model cáº§n thiáº¿t: model táº¡o caption, tokenizer, vÃ  model trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng.
    HÃ m nÃ y Ä‘Æ°á»£c cache Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™.
    """
    # Táº£i model táº¡o caption Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n
    # Äáº£m báº£o tá»‡p 'best_model_with_attention.h5' náº±m cÃ¹ng thÆ° má»¥c
    caption_model = load_model('best_model_with_attention.h5')

    # Táº£i tokenizer
    # Äáº£m báº£o tá»‡p 'tokenizer.pkl' náº±m cÃ¹ng thÆ° má»¥c
    with open('tokenizer.pkl', 'rb') as f:
        tokenizer = pickle.load(f)

    # Táº£i model ResNet50 Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng áº£nh
    # ChÃºng ta chá»‰ láº¥y cÃ¡c lá»›p Ä‘áº¿n trÆ°á»›c lá»›p phÃ¢n loáº¡i cuá»‘i cÃ¹ng
    image_model = ResNet50(weights='imagenet')
    # Táº¡o má»™t model má»›i báº±ng cÃ¡ch láº¥y Ä‘áº§u ra cá»§a lá»›p Ã¡p chÃ³t (thÆ°á»ng lÃ  GlobalAveragePooling2D)
    feature_extractor = tf.keras.Model(inputs=image_model.inputs, outputs=image_model.layers[-2].output)
    
    return caption_model, tokenizer, feature_extractor

# --- HÃ m trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« áº£nh ---
def extract_features(image, model):
    """
    Chuyá»ƒn Ä‘á»•i áº£nh ngÆ°á»i dÃ¹ng táº£i lÃªn thÃ nh vector Ä‘áº·c trÆ°ng báº±ng ResNet50.
    """
    # Thay Ä‘á»•i kÃ­ch thÆ°á»›c áº£nh thÃ nh 224x224 cho phÃ¹ há»£p vá»›i ResNet50
    image = image.resize((224, 224))
    # Chuyá»ƒn áº£nh thÃ nh máº£ng numpy
    image_array = img_to_array(image)
    # ThÃªm má»™t chiá»u Ä‘á»ƒ táº¡o thÃ nh batch size lÃ  1
    image_array = np.expand_dims(image_array, axis=0)
    # Tiá»n xá»­ lÃ½ áº£nh theo cÃ¡ch mÃ  ResNet50 yÃªu cáº§u
    image_array = preprocess_input(image_array)
    # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng
    features = model.predict(image_array, verbose=0)
    return features

# --- HÃ m chuyá»ƒn Ä‘á»•i index thÃ nh tá»« ---
def word_for_id(integer, tokenizer):
    """
    TÃ¬m tá»« tÆ°Æ¡ng á»©ng vá»›i má»™t chá»‰ sá»‘ (integer) trong tokenizer.
    """
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

# --- HÃ m táº¡o Caption (Sá»­ dá»¥ng thuáº­t toÃ¡n Greedy Search) ---
def generate_caption(model, tokenizer, photo_features, max_length):
    """
    Táº¡o ra má»™t cÃ¢u chÃº thÃ­ch tá»« Ä‘áº·c trÆ°ng áº£nh.
    """
    # Báº¯t Ä‘áº§u chuá»—i vá»›i token 'startseq'
    in_text = 'startseq'
    # Láº·p láº¡i Ä‘á»ƒ táº¡o tá»«ng tá»« trong cÃ¢u
    for _ in range(max_length):
        # Chuyá»ƒn chuá»—i hiá»‡n táº¡i thÃ nh dáº¡ng sá»‘
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        # Padding Ä‘á»ƒ chuá»—i cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh
        sequence = pad_sequences([sequence], maxlen=max_length)
        # Dá»± Ä‘oÃ¡n tá»« tiáº¿p theo
        yhat = model.predict([photo_features, sequence], verbose=0)
        # Láº¥y ra tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t
        yhat = np.argmax(yhat)
        # Chuyá»ƒn tá»« dáº¡ng sá»‘ vá» dáº¡ng chá»¯
        word = word_for_id(yhat, tokenizer)
        # Dá»«ng láº¡i náº¿u khÃ´ng tÃ¬m tháº¥y tá»«
        if word is None:
            break
        # ThÃªm tá»« vá»«a dá»± Ä‘oÃ¡n vÃ o chuá»—i
        in_text += ' ' + word
        # Dá»«ng láº¡i náº¿u gáº·p token 'endseq'
        if word == 'endseq':
            break
            
    # Dá»n dáº¹p cÃ¢u káº¿t quáº£
    final_caption = in_text.split()
    final_caption = final_caption[1:-1] # Bá» 'startseq' vÃ  'endseq'
    final_caption = ' '.join(final_caption)
    return final_caption.capitalize() + '.'

# --- Giao diá»‡n ngÆ°á»i dÃ¹ng Streamlit ---

st.title("ğŸ“· TrÃ¬nh táº¡o ChÃº thÃ­ch áº¢nh báº±ng AI")
st.write("Táº£i lÃªn má»™t hÃ¬nh áº£nh vÃ  AI sáº½ tá»± Ä‘á»™ng táº¡o ra má»™t cÃ¢u mÃ´ táº£ cho nÃ³.")

# Táº£i cÃ¡c model cáº§n thiáº¿t vÃ  hiá»ƒn thá»‹ thÃ´ng bÃ¡o chá»
with st.spinner("Äang chuáº©n bá»‹ mÃ´ hÃ¬nh, vui lÃ²ng chá» má»™t chÃºt..."):
    try:
        caption_model, tokenizer, feature_extractor = load_all_models()
        # XÃ¡c Ä‘á»‹nh Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a caption tá»« cáº¥u trÃºc model
        # Input cá»§a model caption thÆ°á»ng lÃ  [features, text_sequence]
        max_caption_length = caption_model.input_shape[1][1]
    except Exception as e:
        st.error(f"Lá»—i khi táº£i model: {e}")
        st.error("Vui lÃ²ng Ä‘áº£m báº£o cÃ¡c tá»‡p `best_model_with_attention.h5` vÃ  `tokenizer.pkl` náº±m Ä‘Ãºng trong thÆ° má»¥c chá»©a file `app.py`.")
        st.stop()


# Widget Ä‘á»ƒ ngÆ°á»i dÃ¹ng táº£i áº£nh lÃªn
uploaded_file = st.file_uploader("Chá»n má»™t tá»‡p áº£nh...", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    # Má»Ÿ vÃ  hiá»ƒn thá»‹ áº£nh
    image = Image.open(uploaded_file).convert("RGB")

    st.image(image, caption='áº¢nh Ä‘Ã£ táº£i lÃªn', use_column_width=True)
    st.write("") # ThÃªm má»™t khoáº£ng trá»‘ng

    # Khi ngÆ°á»i dÃ¹ng nháº¥n nÃºt
    if st.button('Táº¡o chÃº thÃ­ch! âœ¨'):
        with st.spinner("AI Ä‘ang phÃ¢n tÃ­ch hÃ¬nh áº£nh vÃ  suy nghÄ©... ğŸ¤”"):
            # 1. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« áº£nh
            features = extract_features(image, feature_extractor)
            
            # 2. Táº¡o chÃº thÃ­ch tá»« Ä‘áº·c trÆ°ng
            caption = generate_caption(caption_model, tokenizer, features, max_caption_length)
            
            # 3. Hiá»ƒn thá»‹ káº¿t quáº£
            st.success("HoÃ n thÃ nh!")
            st.subheader("ChÃº thÃ­ch Ä‘Æ°á»£c táº¡o:")
            st.markdown(f"## *\"{caption}\"*")

st.markdown("---")
st.write("XÃ¢y dá»±ng bá»Ÿi Gemini | Model: ResNet50 + Attention (LSTM/GRU)")
