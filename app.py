# File: app.py (version 5 - Bu·ªôc d√πng CPU ƒë·ªÉ g·ª° l·ªói)
import os
# !!! THAY ƒê·ªîI QUAN TR·ªåNG !!!
# ƒê·∫∑t d√≤ng n√†y L√äN TR√äN C√ôNG, tr∆∞·ªõc t·∫•t c·∫£ c√°c l·ªánh import c·ªßa tensorflow/keras
# D√≤ng n√†y s·∫Ω v√¥ hi·ªáu h√≥a GPU v√† bu·ªôc TensorFlow ch·∫°y tr√™n CPU.
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

import streamlit as st
from PIL import Image
import numpy as np
import pickle
from gtts import gTTS
import io

# Import c√°c th√†nh ph·∫ßn t·ª´ Keras 3
import tensorflow as tf
import keras
from keras.models import load_model, Model
from keras.utils import img_to_array
from keras.applications.resnet50 import preprocess_input, ResNet50
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Attention 


# --- C·∫§U H√åNH TRANG ---
st.set_page_config(layout="wide", page_title="T·∫°o Ch√∫ th√≠ch ·∫¢nh")

# --- THAM S·ªê V√Ä C√ÄI ƒê·∫∂T ---
MODEL_PATH = 'best_model_with_attention.h5'
TOKENIZER_PATH = 'tokenizer.pkl'
MAX_CAPTION_LENGTH = 41

@st.cache_resource
def load_keras_model_and_tokenizer():
    """T·∫£i model Keras v√† tokenizer, ƒë·ªìng th·ªùi ƒëƒÉng k√Ω l·ªõp Attention c√≥ s·∫µn."""
    try:
        st.info("B·∫Øt ƒë·∫ßu t·∫£i tokenizer...")
        with open(TOKENIZER_PATH, 'rb') as f:
            tokenizer = pickle.load(f)
        st.info("T·∫£i tokenizer th√†nh c√¥ng.")

        custom_objects = {'Attention': Attention}
        
        st.info("B·∫Øt ƒë·∫ßu t·∫£i model ch√≠nh (best_model_with_attention.h5)...")
        model = load_model(MODEL_PATH, custom_objects=custom_objects)
        st.info("T·∫£i model ch√≠nh th√†nh c√¥ng.")
        
        st.info("B·∫Øt ƒë·∫ßu t·∫£i model ResNet50 (tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng)...")
        base_model = ResNet50(weights='imagenet')
        feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)
        st.info("T·∫£i ResNet50 th√†nh c√¥ng.")

        st.success("T·∫£i t·∫•t c·∫£ model v√† tokenizer ho√†n t·∫•t!")
        return model, tokenizer, feature_extractor
    except Exception as e:
        st.error(f"L·ªói nghi√™m tr·ªçng khi t·∫£i model ho·∫∑c tokenizer: {e}")
        st.stop()

# --- C√ÅC H√ÄM X·ª¨ L√ù ---
def preprocess_image_for_resnet(image_pil):
    img = image_pil.resize((224, 224))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    return img

def generate_caption_keras(image_pil, model, tokenizer, feature_extractor):
    in_text = 'startseq'
    preprocessed_img = preprocess_image_for_resnet(image_pil)
    image_features = feature_extractor.predict(preprocessed_img)

    for _ in range(MAX_CAPTION_LENGTH):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=MAX_CAPTION_LENGTH)[0]
        sequence_input = np.array([sequence])
        prediction = model.predict([image_features, sequence_input], verbose=0)
        predicted_word_idx = np.argmax(prediction[0])
        word = tokenizer.index_word.get(predicted_word_idx, None)
        if word is None or word == 'endseq':
            break
        in_text += ' ' + word
    
    final_caption = in_text.replace('startseq ', '').replace(' endseq', '').replace('_', ' ')
    return final_caption.capitalize()

def text_to_speech(text):
    try:
        tts = gTTS(text=text, lang='vi', slow=False)
        audio_fp = io.BytesIO()
        tts.write_to_fp(audio_fp)
        audio_fp.seek(0)
        return audio_fp
    except Exception as e:
        st.error(f"L·ªói khi t·∫°o audio: {e}")
        return None

# --- GIAO DI·ªÜN ---
caption_model, tokenizer, feature_extractor = load_keras_model_and_tokenizer()
st.title("Ô∏èüñºÔ∏è Tr√¨nh t·∫°o Ch√∫ th√≠ch ·∫£nh T·ª± ƒë·ªông")
st.markdown("---")
col1, col2 = st.columns(spec=[1, 1], gap="large")
with col1:
    st.header("1. T·∫£i ·∫£nh c·ªßa b·∫°n l√™n")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt t·ªáp ·∫£nh...", type=['png', 'jpg', 'jpeg'], label_visibility="collapsed")
    image_placeholder = st.empty()
    if uploaded_file:
        image = Image.open(uploaded_file).convert("RGB")
        image_placeholder.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", use_container_width=True)
with col2:
    st.header("2. K·∫øt qu·∫£ ch√∫ th√≠ch")
    if uploaded_file:
        with st.spinner("üß† M√¥ h√¨nh ƒëang suy nghƒ©..."):
            caption_result = generate_caption_keras(image, caption_model, tokenizer, feature_extractor)
        st.info("Ch√∫ th√≠ch ƒë∆∞·ª£c t·∫°o ra l√†:")
        st.markdown(f"### *‚ú® \"{caption_result}\"*") 
        st.markdown("---")
        st.subheader("Nghe ch√∫ th√≠ch")
        with st.spinner("üîä ƒêang t·∫°o √¢m thanh..."):
            audio_data = text_to_speech(caption_result)
        if audio_data:
            st.audio(audio_data, format='audio/mp3')
    else:
        st.warning("Vui l√≤ng t·∫£i ·∫£nh l√™n ·ªü c·ªôt b√™n tr√°i ƒë·ªÉ xem k·∫øt qu·∫£.")
